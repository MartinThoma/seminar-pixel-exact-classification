%!TEX root = vorlage.tex
% Martin Thoma

% definitions
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\section{Traditional Approaches}\label{sec:traditional-approaches}%
Image segmentation algorithms which use traditional approaches, hence don't
apply neural networks and make heavy use of domain knowledge, are wide-spread
in the computer vision community. This section discribes features which can be
used for segmentation in \cref{subsec:features}, gives a very brief overview of
unsupervised, non-semantic segmentation in
\cref{subsec:unsupervised-traditional-segmentation}, describes Markov Random
Fields in \cref{subsec:markov-random-fields}, \glspl{SVM} in
\cref{subsec:trad-SVM} and Random Forests in \cref{subsec:random-forests}.
Pre- and Postprocessing are covered in \cref{subsec:preprocessing-methods} and
\cref{subsec:post-processing-methods}. The respective advantages of the
classifiers are discussed in \cref{subsec:traditional-approaches-discussion}.

This will not cover non-semantic segmentation algorithms. In particular, it
will not cover any algorithm which does not make use of labeled data such as
the Watershed segmentation~\cite{beucher1992morphological} and clustering
algorithms such as $k$-means~\cite{hartigan1975clustering} or mean shift
clustering~\cite{comaniciu2002mean}. However, it should be noted that such
non-semantic segmentation algorithms can be used to build semantic segmentation
algorithms.

Semantic segmentation algorithms store information about the classes they were
trained to segment.

Non-semantic segmentation algorithms try to detect consistant regions and
region boundaries, while semantic segmentation algorithms detect classes.


\subsection{Features}\label{subsec:features}%
The choice of features is very important in traditional approaches and a lot
of effort was done to find good features.

Poselets were used successfully in \cite{bourdev2010detecting,brox2011object}.
Those features rely on manually added extra keypoints. This is easily possible
for well-known image classes like humans. However, it is difficult for classes
like airplanes or ships where the human annotators do not know the keypoints.
Additionally, the keypoints have to be chosen for every single class. There are
strategies to deal with those problems like viewpoint-dependand keypoints.

Image edges and texture patches were proposed in~\cite{brox2011object}.

Other features include:

\begin{itemize}
    \item Pixel color (e.g. 3 features for RGB, 3 features for HSV, 1 feature
          for the grey-value)
\end{itemize}


\subsection{Unsupervised Segmentation}%
\label{subsec:unsupervised-traditional-segmentation}%

\subsubsection{Clustering Algorithms}
% The mean shift algorithm was introduced by~\cite{comaniciu2002mean} for
% segmentation tasks. The algorithm first applies mean shift filtering on the
% original image data and then clusters the remaining points.
%
% TODO: Understand this algorithm. Use \cite{comaniciu2002mean} and
% \cite{pantofaru2005comparison} for it.

% \subsubsubsection{Watershed Algorithm}\label{subsec:watershed}
% \begin{itemize}
%     \item Apply to image intensity gives some long superpixels
%     \item apply to image gradient magnitude gives rounder superpixels
% \end{itemize}

% \subsubsubsection{$k$-Means}\label{subsec:k-means}


\subsubsection{Graph Based Image Segmentation}%
\label{subsec:graph-based-image-segmentation}%
\href{http://cs.brown.edu/~pff/segment/}{cs.brown.edu}
See \cite{felzenszwalb2004efficient}.

TODO: See also \cite{pantofaru2005comparison} for another description.


\subsubsection{Random Walks}
Notes from \cite{meilpa2001learning}:

\begin{itemize}
    \item Normalized Cut (see \cite{shi2000normalized}) seems to be a special
          case of this
    \item Seems to be a \enquote{spectral method}
    \item This is pairwise clustering. Clusters points which are similar and
          optimize e.g. for maximum total intracluster similarity. In contrast,
          to statistical clustering which assumes a probabilistic model which
          generates the data, this only defines a similarity function.
    \item Spectral clustering is a similarity based method. Spectral clustering
          methods use eingenvalues / eigenvectors of the matrix obtained by the
          similarity function.
\end{itemize}


\subsection{Random Decision Forests}\label{subsec:random-forests}
Random Decision Forests were first proposed in~\cite{ho1995random}. This type
of classifier applies a technique called \textit{ensemble learning}, where
multiple classifiers get trained simultaneously on random subspaces of the
feature~space. In the case of Random Decision Forests, the classifiers are
decision trees. A decision tree is a tree where each inner node uses one or
more features to decide in which branch to descend. Each leaf is a class.

A strength of Random Decision Forests compared to many other classifiers is
that the scale of measure of the features (nominal, ordinal, interval, ratio)
can be arbitrary.

Random decision trees were extensively studied in the past 20~years and a
multitude of training algorithms has been proposed (TODO: cite some).
Typically, random decision trees are trained by adding new nodes until each
leaf contains only nodes of a single class or until it is not possible to split
further (TODO: mention that this only
explains Random Decision Forests for classification). This is called a
\textit{stopping criterion}.

There are two typical training modes: \textit{Central axis projection} and
\textit{perceptron training} In training, for each node a hyperplane is
searched which is optimal according to an error function.

TODO: Begging

Random Decision Forests are applied in~\cite{shotton2008semantic} for
segmentation. Textons are used as features there.


\subsection{Markov Random Fields}\label{subsec:markov-random-fields}
% TODO: Probability space?
A \Gls{MRF} is a tupel $(G, X)$, where $G=(V,E)$ is an undirected graph and
$X=(X_V)_{v \in V}$ is a set of random variables which satisfy the pairwise
Markov property, the local Markov property and the global Markov property:

% https://en.wikipedia.org/wiki/Markov_random_field#Definition
% \begin{itemize}
%     \item \textbf{Pairwise Markov property}: $X_u \independent X_v | X_{V \setminus \Set{u, v}}$ if $\Set{u,v} \notin E$
%     \item \textbf{Local Markov property}: $X_u \independent X_{V \setminus \Set{v | v \text{is } u \text{ or a neighbor of } u}} | X_{\text{neighbors of } v}$
%     \item \textbf{Global Markov property}: $X_A \independent X_B | X_S$ where
%           every path from a node in $A$ to a node in $B$ passes through $S$
% \end{itemize}

% is a probabilisitic model. They have the Markov property which is
% typically displayed in an undirected graph which shows dependencies between the
% random variables.

\Glspl{CRF} and Boltzmann Machines are a variations of Markov random fields.

% http://www.mathunion.org/ICM/ICM1986.2/Main/icm1986.2.1496.1517.ocr.pdf
%
% Segmentation of brain MR images through a hidden Markov random field model
% and the expectation-maximization algorithm \cite{zhang2001segmentation}

% In short: specify locally and model globally.

% define only local properties. allows by transitivity to get a model
% for global properties.

% definition:

% A \gls{MRF} is a countable set of random variables. In the task of segmentation,
% they are often index-based spacial positions.

% \begin{itemize}
%     \item What is your set of random models? (e.g. Ising model: one for each pixel)
%           \begin{itemize}
%               \item Irving-Model: \cite{boykov2000interactive}
%               \item Potts-Model: \cite{boykov2001fast}
%           \end{itemize}
% \end{itemize}

% On those, define an undirected hypergraph $G(V, E)$. $V$ are pixels, $E$ is
% typically defined by neighboring pixels.

% good: 

% \begin{itemize}
%     \item Convenient modeling:Just write down energie function (in some cases)
%           do define entire model
%     \item fast inference (only 1 or 2 variants)
%     \item sometimes good performance
% \end{itemize}

% bad:

% \begin{itemize}
%     \item learning is difficult
% \end{itemize}



% ---


From \cite{yang2012layered}:

> In contrast, semantic segmentation models have largely been built on top of
Markov Random Field (MRF) models which enforce smoothness across pixel labels

\begin{itemize}
    \item X. He, R. Zemel, and M. Carreira-Perpinan, “Multiscale Conditional
          Random Fields for Image Labeling,” Proc. IEEE CS Conf. Computer
          Vision and Pattern Recognition, vol. 2, 2004.
    \item A. Torralba, K. Murphy, and W. Freeman, “Contextual Models for
          Object Detection Using Boosted Random Fields,” Proc. Advances in
          Neural Information Processing Systems, 2004.
    \item S. Kumar and M. Hebert, “A Hierarchical Field Framework for
          Unified Context-Based Classification,” Proc. 10th IEEE Int’l Conf.
          Computer Vision, vol. 2, 2005.
    \item \Glspl{CRF} were applied in \cite{shotton2006textonboost}.
    \item Z. Tu, “Auto-Context and Its Application to High-Level Vision
          Tasks,” Proc. IEEE Conf. Computer Vision and Pattern Recognition,
          2008.
\end{itemize}


\subsection{SVMs}\label{subsec:trad-SVM}%

\Glspl{SVM} are used by \cite{yang2012layered}.


\subsection{Pre-processing methods}\label{subsec:preprocessing-methods}%
A typical image is opened in RGB color space, but depending on the classifier
and the problem another color space might result in better segmentations. RGB,
YcBcr, HSL, Lab and YIQ are some examples used by \cite{cohen2015memory}.

\Gls{PCA} is applied by~\cite{chen2011pixel} to reduce the dimensionality of
the feature space.

\subsection{Post-processing methods}%
\label{subsec:post-processing-methods}%
Post-processing methods are an important part of traditional pixel-level
segmentation approaches. Active contour models are one example of a
post-processing method~\cite{kass1988snakes}.

A refinement of the found segmentation can be obtained by adjusting a found
semantic segmentation to match close edges. This was used
in~\cite{brox2011object} with an ultra-metric contour map
Map~\cite{arbelaez2009contours}


\subsection{Discussion}%
\label{subsec:traditional-approaches-discussion}%
According to \cite{pantofaru2005comparison}, the mean shift algorithm produces
segmentations that correspond well to human perception, but it is sensitive to
its parameters. Depending on them, different granularities of the segmentation
can be achieved. \cite{pantofaru2005comparison} draws the conclusion, that
the segmentations found by the graph based image segmentation approach
in \cite{felzenszwalb2004efficient} are inferior to the segmentations found
by the mean shift algorithm described in \cite{comaniciu2002mean}.
