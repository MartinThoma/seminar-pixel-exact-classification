%!TEX root = vorlage.tex
% Marvin Teichmann
\clearpage

%TODO: Type of MLP

\section{Convolution Neuronal Networks for Computer Vision Tasks}

In order to reasonable train deep models on the high dimensional image data we need models which contains strong prior knowledge. Having prior knowledge about image data allows us to dramatically reduce the capacity (i.e. amount of parameters) without sacrificing much accuracy. CNNs rely on the following two strong assumptions.

\begin{enumerate}
    \item translation invariance and stationarity of statistics
	\item locality of pixel dependencies
\end{enumerate}

Stationarity of statistics is archived by applying a translation invariance functions on each layer. Locality of pixel dependencies is accomplish by making the kernel of this function only depend on a relative small and dense reception field. 

\subsection{Definitions and Notation}

Multi-Layer perceptrons operate in layer, each of shape $h \times w \times d$, where $h$ and $w$ are spatial coordinates and $d$ is the channel size. Let $x_{ij} \in R^d$ be the data of Layer $l$, than Layer $l+1$, given by $y_{ij} \in R^{d'}$ is given by computing a layer function 
\begin{equation*}
y_{nm} := F_{nm} (\{ x_{ij} \}_{0 \leq i \leq h, 0 \leq j \leq w}   )
\end{equation*}

In CNNs $F_{ij}$ is chosen to be translation invariant. $F_{ij}$ can therefore be described by a kernel  operation $f: R^k \rightarrow R$. The meta-parameter $k$ is called \emph{kernel size}, it usually has shape $k = n \times n$ with $n << h,w$.  The function $f$ is than applied to every location in a sliding-window fashion. Sometimes $s$ pixel are skipped in each dimension resulting in a down-sampling of factor $s$. The meta-parameter $s$ is called \emph{stride}. Hence we obtain

\begin{align*}
y_{ij} &:= F_{nm} (\{ x_{ij} \}_{0 \leq i \leq h, 0 \leq j \leq w}   ) \\&\; = f_{ks} (\{x_{s \cdot n + i, s \cdot m + j}  \}_{0 \leq i,j \leq k} ).
\end{align*}

Layer $l+1$ has hence shape $(h - k)/s \times (w-k)/s \times d'$, where $d'$ correspond to the number of filter applied. \emph{Padding} can be applied in order to avoid the loss of information at the border of the feature map in each layer. The output shape than is $h/s \times w/s \times d'$.

\subsection{Layer Types}

CNNs are build using three different layer types. Namely these are \emph{convolutional}, \emph{pooling} and \emph{activation} layers.

\subsubsection{Convolutional Layer}

Convolutional layer implement a learnable convolution operation inside the neural network model. To archive that $f_{ks}$ is chosen to be linear function (i.e. a matrix). Observe that this can be viewed as a special case of an MLP, where curtained weights are enforced to be equal or fixed to be zero. The parameters can therefore be learned using a back-propagation approach. In computer graphics convolutions are a very important tool. They can be used for a variety of tasks including edge and area detection, contrast sharpening and image blurring. Having learnable convolution kernels is therefore a very powerful tool. In convolutional layers stride is usually choose to be $s=1$ , unless the kernel-size is relatively big ($k \geq 7$).  (cite: AlexNet, VGG, GoogLeNET (LeNet?)).

%Recent networks.
 
 \subsubsection{Pooling Layer}

The pooling layer applies non-learnable function, which collects a summary statistic about a region of the feature map. Typical choices are max- or mean-pooling, computing the corresponding function on its input region. 

For the pooling layer typically $s$ is choose to be $k$ (VGG, GoogLeNet, OverFeat),  although overlapping pooling has been successfully applied (Alexnet). Typical choices for the kernes size include $2 \times 2$ or $3 \times 3$. (cite).

Applying pooling has two  advantages: Firstly it naturally reduces the spatial dimension enabling the network to learn more compact representation if the data and decreasing the amount of parameters in the succeeding layers. Secondly it introduces robust translation invariant. Minor shifts in the input data will not result in the same activation after pooling. The drawback of pooling however is, that fine-grained spatial information are lost in the process. This is a negligible disadvantage for non-spatial tasks such as classification but comes severe in segmentation. 

 \subsubsection{Activation Layers}
 
 To enable the CNN to learn non-linear function it is crucial, that some kind of non-linearity is applied between layers. Otherwise the Network is equivalent to the concatenation of linear functions, hence a linear function itself which can be equally represented using a single layer. Non-linearities are usually one-dimensional functions $f: R \rightarrow R$, applied to each coordinate individually, hence they can be viewed as an kernel operation with size $k=1$ and stride $s=1$ in the above notation. 
 
Classical choices for nonlinearities are the hyperbolic tangent \emph{tanh} and the sigmoid function $f(x) = (1- exp(-x))^{-1}$. Recently ReLU Nonlinearities (AlexNet, Bolzmann) have gained a lot of popularity (cite). ReLU Nonlinearities have several advantages over traditional nonlinearities. Firstly they are very fast and efficient to compute on GPU. Secondly it does not suffer from the gradient vanishing problem and lastly empirical results show, that training converges several times faster than with other Nonlinearities.

\subsubsection{Fully Connected Layers}

\label{sec:fully_connected}

\subsection{Relevant Network Architectures}


\subsection{Fully Convolution Approach}

As described in Section () Semantic Segmentation can be views as a spatial version of classification, where a classification model can be transfered into a segmentation model using a sliding window approach. This is a practical solution, as sliding window like computation can be carried out very efficiently in ConvNets. Most CNN based segmentation approaches are using this insight to build models on the shoulders of AlexNet and its deeper successors. 

\subsection{Sliding Window efficiency in CNNs}

%TODO: Directly copied from paper: overfeat

In contrast to many sliding-window approaches that compute an entire pipeline for each window that input one at a time, ConvNets are inherently efficient because they naturally share computations common to overlapping regions. 

%TODO: Directly copied from paper: overfeat

This is because the convolution properties are computational traceable. Let $F, G$ be given as convolution using kernel function $f,g$ with sizes $k,k'$ and stride $s,s'$, respectively. Than $F \circ G$ is obtained by convolution with $f \circ g$, which has kernel size $k + (k-1) s'$ using stride $s \cdot s'$. A classification ConvNet with input layer of size $k \times k$, output of size $1 \times 1$ therefor computes a convolution with kernel size $k^2$ and stride $s$ equal to the product of strides inside the network. If fully connected layer are transformed into convolutional layer (as descripted in ...), it is possible to apply this network on an input image of size $n \times m$ with $n,m \geq k$ and to obtain an output equal to a sliding window with stride $s$. 

The main drawback of this method is, that the stride $s$ becomes quite large ($36$ in OverFeat, a network designed to have small strides). There are two approaches to overcome this problem:

\subsubsection{Shift-and-stitch} The idea of this approach is to feed each layer with stride $s$ with $s^2$ shifted versions of the input. This basically neglects the computational advantage of strides, but keeps the statistical relevant effects. Fast-scanning (cite) descripes a trick to efficiently perform this computation.  This trick is applied in [overfeat, huval].

\subsubsection{Deconvolution} Alternatively one can add deconvolution layer on top of the ConvNet architecture. This enables the network to learn to upsample the coarse representation obtained by the sliding-window. This approach is introduced in (cite) and since used in a variety of publications (SegNET, ...)

One advantage of the deconvolution approach is, that the original ConvNet can be trained using data labeled for the classification task, only the deconvolution layer .

\subsection{FCNN Ar}

% Explain CNN
% Assumption

\section{Neural Networks for Segmentation}




\clearpage